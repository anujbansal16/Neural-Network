{
  "cells": [
    {
      "metadata": {
        "_uuid": "5b6fe18165db2e3add5bcb8f833fa86cdc3d6e94"
      },
      "cell_type": "markdown",
      "source": "# Assignment-5"
    },
    {
      "metadata": {
        "_uuid": "d32a6f32c3f360ebdf59a16e7757a34c9581c7b1"
      },
      "cell_type": "markdown",
      "source": "### Import necessary libraries"
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DlYwwlVNYTQJ",
        "trusted": true,
        "_uuid": "d78c30fb1ae3fbe28508c51b0388e9a6e7ac1f54"
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3El9g_GXYayN",
        "trusted": true,
        "_uuid": "c902470f1c4c7ca9b8a9f330a312f7869c646e73"
      },
      "cell_type": "code",
      "source": "def splitTrainTest(data,percent):\n    total=len(data)\n    trainTotal=int(total*percent*0.01)\n    testTotal=total-trainTotal\n    return (data[0:trainTotal],data[trainTotal:total])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d412f765569938aaf02b03aace0c70f774c95960"
      },
      "cell_type": "markdown",
      "source": "### Layer Class\nThis class's object represent the layers in neural network. It stores the number of neurons in each layers, activations, activation function associated with each layer and their weight vector (initialize on gaussian distribute with mean =0 and std deviation=1)."
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5q_LrpcRa4sm",
        "trusted": true,
        "_uuid": "296710bb41c7f337c77f17726e26c77c4c04e2c0"
      },
      "cell_type": "code",
      "source": "class Layer:\n    def __init__(self,nNodesCurrent, nNodesNext, activationF):\n        self.nodesNo=nNodesCurrent\n        self.activations = np.zeros([nNodesCurrent,1])\n        self.activationF=activationF\n        if nNodesNext==0:\n            self.weights=None\n        else:\n            self.weights=np.random.normal(0, 1, size=(nNodesCurrent,nNodesNext))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "56465628011c10a40ed62d4b7dc9603b334d7664"
      },
      "cell_type": "markdown",
      "source": "### Neural Network Class\nClass of neural network to perform multiclass classification using the cross entropy as cost function and softmax as probabilty distribution activation function.\n\nThis class perform following tasks:\n* Initialized the number of layers in neural network and create **Layer Class** object.\n* Train the neural network on batches of inputs there by performing the forward and backward propogation using below helper methods.\n\n#### Methods:\n1. **Forward Propogation** : Perform the forward propogation, calculate and stores activations at each of the \nlayer.\n$$ z_1= w_1^TX $$\n$$ a_1= f _1(z_1) $$\n$$ z_2= w_2^Ta_1 $$\n$$ \\hat{y}= a_2= f_2(z_2) $$\n2. **Calculate Error** : Here we calculate the cross entropy error of our neural network on the updated activations. This updation in activation take place after the updation of weights in gradient decent algorithm in backpropogation.\n<br>\nFor multiclass classification we use the below cross entropy cost function:\n$$ J =  -\\sum\\limits_{i} y \\log \\; \\hat{y} $$\n3. **Backward Propogation** : Here, we differentiate the cost function to minimize it, and find the optimal values of parameters ie weights at each of the layer.\n$$\\frac{\\partial J}{\\partial w_2 } =  \\frac{\\partial J}{\\partial \\hat{y} }  * \\frac{\\partial \\hat{y}}{\\partial z_2 }  * \\frac{\\partial z_2}{\\partial w_2 }   $$\n<br>\nalso for cross entropy as a cost function and softmax as $f_2(z_2)$ , \n$$ \\delta_3  =  \\frac{\\partial J}{\\partial \\hat{y} }  * \\frac{\\partial \\hat{y}}{\\partial z_2 } = (y_p - y_a)  $$\n<br>\n$$ \\therefore   \\frac{\\partial J}{\\partial w_2 }= (\\hat{y} - y) * a_1$$\n<br><br>\nSimilarly to calculate parital derivative w.r.t weights of inner layers, we can use the chain rule\n$$   \\frac{\\partial J}{\\partial w_1 } = x^T *  \\delta_3 *  w_2 *  f^1 (z_1)      $$\n\n4. **Get Accuracy**: This function will return the accuracy of our neural network on multiclass classification.\n\n<hr>\n\n**Some important activations functions and their derivatives used in our network**\n**Softmax**\n<br>\n$$\\sigma (z)_j = \\frac{e^{z_j}}{\\sum^K_{k=1} e^{z_j}}$$\n**Sigmoid**\n<br>\n$$sigmoid(x) = \\frac{1}{1+\\epsilon ^ {-x}}$$\n$$\\frac{\\partial sigmoid(x)}{\\partial x} = sigmoid(x) * ( 1- sigmoid(x))$$\n\n**Relu**\n<br>\n$$relu(x) = \\max{(0,x)}$$\n<br>\n$$\\frac{\\partial relu(x)}{\\partial x} = 1 \\;\\;\\;\\; if x>0  \\\\ 0 \\;\\;\\; elsewhere$$\n\n**tanh**\n<br>\n$$tanh(x) = \\tanh{(x)}$$\n<br>\n$$\\frac{\\partial tanh(x)}{\\partial x} = 1 - \\tanh^2{(x)}$$"
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "foJcY9UbkqRp",
        "trusted": true,
        "_uuid": "e17771a8b78f252040509525f72dd6c69605adb8"
      },
      "cell_type": "code",
      "source": "class NeuralNet:\n    def __init__(self, totalLayers, noNodesList, activationFunctions):\n        self.totalLayers=totalLayers\n        self.noNodesList=noNodesList\n        self.layers = []\n        for i in range(totalLayers):\n            currentLayerNodes=noNodesList[i]\n            if i!=totalLayers-1:\n                nextLayerNodes=noNodesList[i+1]\n                ith_Layer=Layer(currentLayerNodes,nextLayerNodes,activationFunctions[i])\n            else:\n                ith_Layer=Layer(currentLayerNodes,0,activationFunctions[i])\n            self.layers.append(ith_Layer)#append output layer as none\n\n    def trainNetwork(self, data,outputLabels, batchSize, epochs, learningRate):\n        self.learningRate=learningRate\n        self.batchSize=batchSize;\n        \n        #normalize data\n#         data=((data-data.min(axis=0))/(data.max(axis=0)-data.min(axis=0)))\n        data=data/255\n        \n        for x in range(epochs):\n            i=0  \n            while i<len(data):\n                self.error=0\n                self.forwardPropo(data[i:i+batchSize])#input\n                self.calculateError(outputLabels[i:i+batchSize])#output\n                self.backwardPropo(outputLabels[i:i+batchSize])\n                i+=batchSize\n            self.error /= batchSize\n            print(\"Epoch \",x,\"->Error: \", self.error)\n        \n          \n    def forwardPropo(self, inputs):\n        self.layers[0].activations =inputs\n        for i in range(self.totalLayers-1):\n            temp=np.matmul(self.layers[i].activations,self.layers[i].weights)  \n            if self.layers[i+1].activationF == \"sigmoid\":\n                self.layers[i+1].activations = self.sigmoid(temp)\n            elif self.layers[i+1].activationF == \"softmax\":\n                self.layers[i+1].activations = self.softmax(temp)\n            elif self.layers[i+1].activationF == \"relu\":\n                self.layers[i+1].activations = self.relu(temp)\n            elif self.layers[i+1].activationF == \"tanh\":\n                self.layers[i+1].activations = self.tanh(temp)\n            else:\n                self.layers[i+1].activations = temp\n        \n    def calculateError(self,labels):\n        if len(labels[0]) != self.layers[self.totalLayers-1].nodesNo:\n            print (\"Error: Label is not of the same shape as output layer.\")\n            print(\"Label: \", len(labels), \" : \", len(labels[0]))\n            print(\"Out: \", len(self.layers[self.totalLayers-1].activations), \" : \", len(self.layers[self.totalLayers-1].activations[0]))\n            return\n        self.error += np.negative(np.sum(np.multiply(labels, np.log(self.layers[self.totalLayers-1].activations))))\n    \n    def backwardPropo(self, labels):\n        targets = labels\n        i = self.totalLayers-1\n        y = self.layers[i].activations\n        \n        delta=(y-targets)\n        deltaw = np.dot(self.layers[i-1].activations.T, delta)/self.batchSize\n        new_weights = self.layers[i-1].weights - self.learningRate * deltaw\n        for i in range(i-1, 0, -1):\n            if self.layers[i].activationF==\"sigmoid\":\n                prime= self.sigmoid_derivative(np.matmul(self.layers[i-1].activations,self.layers[i-1].weights))\n            elif self.layers[i].activationF==\"relu\":\n                prime= self.relu_derivative(np.matmul(self.layers[i-1].activations,self.layers[i-1].weights))\n            elif self.layers[i].activationF==\"tanh\":\n                prime= self.tanh_derivative(np.matmul(self.layers[i-1].activations,self.layers[i-1].weights))\n            \n            delta=np.multiply(prime,delta.dot(self.layers[i].weights.T))\n            deltaw = np.dot(self.layers[i-1].activations.T, delta)/self.batchSize\n\n            self.layers[i].weights = new_weights\n            new_weights = self.layers[i-1].weights - self.learningRate * deltaw\n        self.layers[0].weights = new_weights\n            \n    def getAccuracy(self, inputs, labels):\n        inputs=inputs/255\n        self.batchSize = len(inputs)\n        self.forwardPropo(inputs)\n        a = self.layers[self.totalLayers-1].activations\n        print(len(a))\n        total=0\n        correct=0\n        for i in range(len(a)):\n            total += 1\n            al = a[i].tolist()\n            if labels[i][al.index(max(al))] == 1:\n                correct += 1\n        print(correct)\n        print(\"Accuracy: \", correct*100/total)\n    \n    def sigmoid(self, x):\n        return np.divide(1, np.add(1, np.exp(np.negative(x))))\n    \n    def sigmoid_derivative(self,x):\n        return (self.sigmoid(x)*(1-self.sigmoid(x)))\n    \n    def relu(self, x):\n        return (x/700) * (x > 0)\n    \n    def relu_derivative(self,X):\n        return 1. * (X > 0)\n    \n    def softmax(self, x):\n        exp = np.exp(x)\n        if isinstance(x[0], np.ndarray):\n            return exp/np.sum(exp, axis=1, keepdims=True)\n        else:\n            return exp/np.sum(exp, keepdims=True)\n\n    def tanh(self, x):\n        return np.tanh(x)\n    \n    def tanh_derivative(self,x):\n        return 1.0 - np.tanh(x) ** 2",
      "execution_count": 283,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f382776340b5b2ceb8352f6cd8511ad0835d8a1e"
      },
      "cell_type": "code",
      "source": "def getOneHotLabels(data,k):\n    one_hot_labels = np.zeros((len(data), k))\n    for i in range(len(data)):  \n        one_hot_labels[i,data[i,0]] = 1\n    return one_hot_labels",
      "execution_count": 241,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SQ5KUVammIaS",
        "trusted": true,
        "_uuid": "29bb65a597a19af437582c16d930790b7a792a83"
      },
      "cell_type": "code",
      "source": "data=pd.read_csv(\"../input/apparel-trainval.csv\").values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c1c1d1ec4f2f78ad27a9de4a66a4a1d185177d29"
      },
      "cell_type": "markdown",
      "source": "### Question-1 Part-2"
    },
    {
      "metadata": {
        "_uuid": "c1bc2859e42d7b0ed0315189773705eca5611969"
      },
      "cell_type": "markdown",
      "source": "**Sigmoid**"
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Ievzm6hptydg",
        "trusted": true,
        "_uuid": "55c624c07d44df9d4ae46f7c997f640163305b64",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "train,test=splitTrainTest(data,80)\noneHotLabelsTrain=getOneHotLabels(train,10)\noneHotLabelsTest=getOneHotLabels(test,10)\ntrainInputs=train[:,1:]\ntestInputs=test[:,1:]\nprint(\"Neural network with sigmoid activation function in hidden layers\")\n\nnumberofLayers=4\nnoofneurons=[784,16,16,10]\nactivationFunctions=[None,\"sigmoid\",\"sigmoid\",\"softmax\"]\nbatchSize=64\nepochs=50\nlearningRat=0.1\n\n#NeuralNet(noLayers, noNeurons in each layer, activationFunctions)\nnn=NeuralNet(numberofLayers,noofneurons,activationFunctions)\n\nnn.trainNetwork(trainInputs,oneHotLabelsTrain,batchSize,epochs,learningRat)\nnn.getAccuracy( testInputs, oneHotLabelsTest)",
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Neural network with sigmoid activation function in hidden layers\nEpoch  0 ->Error:  1.1444362710733365\nEpoch  1 ->Error:  0.8908223800291348\nEpoch  2 ->Error:  0.8233232467751349\nEpoch  3 ->Error:  0.7930685779650962\nEpoch  4 ->Error:  0.7710560795228129\nEpoch  5 ->Error:  0.7498884656331919\nEpoch  6 ->Error:  0.7273381097528101\nEpoch  7 ->Error:  0.7042487708341048\nEpoch  8 ->Error:  0.6813432143368299\nEpoch  9 ->Error:  0.6612763580214069\nEpoch  10 ->Error:  0.6446275418117886\nEpoch  11 ->Error:  0.6309052730757667\nEpoch  12 ->Error:  0.6199287702205996\nEpoch  13 ->Error:  0.6107823543078843\nEpoch  14 ->Error:  0.6015650332029695\nEpoch  15 ->Error:  0.5923679680194792\nEpoch  16 ->Error:  0.5836847893284267\nEpoch  17 ->Error:  0.5751551321748132\nEpoch  18 ->Error:  0.566363783765619\nEpoch  19 ->Error:  0.5570908098102729\nEpoch  20 ->Error:  0.5470884223915831\nEpoch  21 ->Error:  0.5363040433133341\nEpoch  22 ->Error:  0.5253838983021238\nEpoch  23 ->Error:  0.5153271903378908\nEpoch  24 ->Error:  0.506392563173309\nEpoch  25 ->Error:  0.49835084699654963\nEpoch  26 ->Error:  0.4910319234644267\nEpoch  27 ->Error:  0.4844268126461906\nEpoch  28 ->Error:  0.4785790773793994\nEpoch  29 ->Error:  0.4734616455994165\nEpoch  30 ->Error:  0.46893693824090865\nEpoch  31 ->Error:  0.46479739538047465\nEpoch  32 ->Error:  0.4608536416955539\nEpoch  33 ->Error:  0.45699430452080175\nEpoch  34 ->Error:  0.4531730735654185\nEpoch  35 ->Error:  0.44936950387079283\nEpoch  36 ->Error:  0.4455712651428344\nEpoch  37 ->Error:  0.441777456125154\nEpoch  38 ->Error:  0.43800193323919384\nEpoch  39 ->Error:  0.43426893764670316\nEpoch  40 ->Error:  0.4306074640835977\nEpoch  41 ->Error:  0.4270457918449293\nEpoch  42 ->Error:  0.42361184238072697\nEpoch  43 ->Error:  0.4203461185999817\nEpoch  44 ->Error:  0.41730319141416583\nEpoch  45 ->Error:  0.4145214292108536\nEpoch  46 ->Error:  0.41199484987454094\nEpoch  47 ->Error:  0.40968122743892166\nEpoch  48 ->Error:  0.4075258271014086\nEpoch  49 ->Error:  0.40547443081625933\n12000\n9877\nAccuracy:  82.30833333333334\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "9ccdbe974cf8b9644a04fd700dced65302c64f1d"
      },
      "cell_type": "markdown",
      "source": "**Tanh**"
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Ievzm6hptydg",
        "trusted": true,
        "_uuid": "55c624c07d44df9d4ae46f7c997f640163305b64"
      },
      "cell_type": "code",
      "source": "train,test=splitTrainTest(data,80)\noneHotLabelsTrain=getOneHotLabels(train,10)\noneHotLabelsTest=getOneHotLabels(test,10)\ntrainInputs=train[:,1:]\ntestInputs=test[:,1:]\nprint(\"Neural network with tanh activation function in hidden layers\")\n\nnumberofLayers=4\nnoofneurons=[784,16,16,10]\nactivationFunctions=[None,\"tanh\",\"tanh\",\"softmax\"]\nbatchSize=64\nepochs=50\nlearningRat=0.1\n\n#NeuralNet(noLayers, noNeurons in each layer, activationFunctions)\nnn=NeuralNet(numberofLayers,noofneurons,activationFunctions)\n\nnn.trainNetwork(trainInputs,oneHotLabelsTrain,batchSize,epochs,learningRat)\nnn.getAccuracy( testInputs, oneHotLabelsTest)",
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Neural network with tanh activation function in hidden layers\nEpoch  0 ->Error:  0.9147561093379838\nEpoch  1 ->Error:  0.7601456485701445\nEpoch  2 ->Error:  0.6674523294338822\nEpoch  3 ->Error:  0.6600319876154879\nEpoch  4 ->Error:  0.619981507480916\nEpoch  5 ->Error:  0.6191465188244452\nEpoch  6 ->Error:  0.6242408257106333\nEpoch  7 ->Error:  0.6441535465193587\nEpoch  8 ->Error:  0.657844591786037\nEpoch  9 ->Error:  0.6426668242482998\nEpoch  10 ->Error:  0.6405906354616409\nEpoch  11 ->Error:  0.6340736362465693\nEpoch  12 ->Error:  0.6366770445885462\nEpoch  13 ->Error:  0.6397543444264681\nEpoch  14 ->Error:  0.6355432380601799\nEpoch  15 ->Error:  0.6393952941441227\nEpoch  16 ->Error:  0.6370275001477713\nEpoch  17 ->Error:  0.6318105346596896\nEpoch  18 ->Error:  0.610671360339365\nEpoch  19 ->Error:  0.5937647955716299\nEpoch  20 ->Error:  0.5716779396451035\nEpoch  21 ->Error:  0.5626718533952708\nEpoch  22 ->Error:  0.5582246468967483\nEpoch  23 ->Error:  0.5530235268787245\nEpoch  24 ->Error:  0.5513297036504091\nEpoch  25 ->Error:  0.549532846692034\nEpoch  26 ->Error:  0.5459501479397788\nEpoch  27 ->Error:  0.5367636981819672\nEpoch  28 ->Error:  0.5233265941113099\nEpoch  29 ->Error:  0.5280377271139015\nEpoch  30 ->Error:  0.5274865868268004\nEpoch  31 ->Error:  0.5206437042738652\nEpoch  32 ->Error:  0.5096353338427405\nEpoch  33 ->Error:  0.5014894592053141\nEpoch  34 ->Error:  0.4953010267089034\nEpoch  35 ->Error:  0.490309255167657\nEpoch  36 ->Error:  0.4856945275162476\nEpoch  37 ->Error:  0.48260435032070315\nEpoch  38 ->Error:  0.47837476630246256\nEpoch  39 ->Error:  0.46917048812595674\nEpoch  40 ->Error:  0.4617597307733785\nEpoch  41 ->Error:  0.45435617272618756\nEpoch  42 ->Error:  0.4494409598642589\nEpoch  43 ->Error:  0.44904145982929555\nEpoch  44 ->Error:  0.44159539541933057\nEpoch  45 ->Error:  0.43925213530959817\nEpoch  46 ->Error:  0.4339229070481271\nEpoch  47 ->Error:  0.4313044126869665\nEpoch  48 ->Error:  0.4280459754190714\nEpoch  49 ->Error:  0.4252631915982861\n12000\n9629\nAccuracy:  80.24166666666666\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "49acb05f48981d1aa92fd24dc9797019ad2adc9d"
      },
      "cell_type": "markdown",
      "source": "**ReLU**"
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Ievzm6hptydg",
        "trusted": true,
        "_uuid": "55c624c07d44df9d4ae46f7c997f640163305b64"
      },
      "cell_type": "code",
      "source": "train,test=splitTrainTest(data,80)\noneHotLabelsTrain=getOneHotLabels(train,10)\noneHotLabelsTest=getOneHotLabels(test,10)\ntrainInputs=train[:,1:]\ntestInputs=test[:,1:]\nprint(\"Neural network with reLU activation function in hidden layers\")\n\nnumberofLayers=4\nnoofneurons=[784,16,16,10]\nactivationFunctions=[None,\"relu\",\"relu\",\"softmax\"]\nbatchSize=64\nepochs=50\nlearningRat=0.1\n\n#NeuralNet(noLayers, noNeurons in each layer, activationFunctions)\nnn=NeuralNet(numberofLayers,noofneurons,activationFunctions)\n\nnn.trainNetwork(trainInputs,oneHotLabelsTrain,batchSize,epochs,learningRat)\nnn.getAccuracy( testInputs, oneHotLabelsTest)\n",
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Neural network with reLU activation function in hidden layers\nEpoch  0 ->Error:  1.1200639439802258\nEpoch  1 ->Error:  0.7768701455401835\nEpoch  2 ->Error:  0.6826118613949433\nEpoch  3 ->Error:  0.601097727277558\nEpoch  4 ->Error:  0.5401174423527499\nEpoch  5 ->Error:  0.4939050145023782\nEpoch  6 ->Error:  0.44880891295912295\nEpoch  7 ->Error:  0.41630738826816527\nEpoch  8 ->Error:  0.38543007848742716\nEpoch  9 ->Error:  0.3602471454015399\nEpoch  10 ->Error:  0.33890628990100635\nEpoch  11 ->Error:  0.31979703725485953\nEpoch  12 ->Error:  0.30486966629376494\nEpoch  13 ->Error:  0.2939486849104042\nEpoch  14 ->Error:  0.2865041112838718\nEpoch  15 ->Error:  0.2784787397629279\nEpoch  16 ->Error:  0.2722440148543084\nEpoch  17 ->Error:  0.2653483233853403\nEpoch  18 ->Error:  0.2599242885009695\nEpoch  19 ->Error:  0.25535208624075295\nEpoch  20 ->Error:  0.25097954213408186\nEpoch  21 ->Error:  0.2456668034194958\nEpoch  22 ->Error:  0.242839050825094\nEpoch  23 ->Error:  0.24148325900938936\nEpoch  24 ->Error:  0.23769556720242865\nEpoch  25 ->Error:  0.2344556021130012\nEpoch  26 ->Error:  0.23367321498569366\nEpoch  27 ->Error:  0.23127521364656511\nEpoch  28 ->Error:  0.23044337809706453\nEpoch  29 ->Error:  0.22793527018936965\nEpoch  30 ->Error:  0.2267753243958221\nEpoch  31 ->Error:  0.22712674295432983\nEpoch  32 ->Error:  0.22654442402156827\nEpoch  33 ->Error:  0.22645250334545086\nEpoch  34 ->Error:  0.22597920162531154\nEpoch  35 ->Error:  0.2273182665783426\nEpoch  36 ->Error:  0.2269757369053713\nEpoch  37 ->Error:  0.22588102882386307\nEpoch  38 ->Error:  0.22445937116082115\nEpoch  39 ->Error:  0.22182336230644134\nEpoch  40 ->Error:  0.22052547967807992\nEpoch  41 ->Error:  0.2196583685213692\nEpoch  42 ->Error:  0.21767965543826703\nEpoch  43 ->Error:  0.2160263003822809\nEpoch  44 ->Error:  0.21550493845385832\nEpoch  45 ->Error:  0.21546003816339762\nEpoch  46 ->Error:  0.21545841504914462\nEpoch  47 ->Error:  0.21528706257035327\nEpoch  48 ->Error:  0.21117003613549049\nEpoch  49 ->Error:  0.21185178632474888\n12000\n10319\nAccuracy:  85.99166666666666\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "009118e7f3e0922109fb5daebe69e05e0b882e7b"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Ass5.ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}