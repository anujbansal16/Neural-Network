{
  "cells": [
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DlYwwlVNYTQJ",
        "trusted": true,
        "_uuid": "d78c30fb1ae3fbe28508c51b0388e9a6e7ac1f54"
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\n",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3El9g_GXYayN",
        "trusted": true,
        "_uuid": "c902470f1c4c7ca9b8a9f330a312f7869c646e73"
      },
      "cell_type": "code",
      "source": "def splitTrainTest(data,percent):\n    total=len(data)\n    trainTotal=int(total*percent*0.01)\n    testTotal=total-trainTotal\n    return (data[0:trainTotal],data[trainTotal:total])",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5q_LrpcRa4sm",
        "trusted": true,
        "_uuid": "296710bb41c7f337c77f17726e26c77c4c04e2c0"
      },
      "cell_type": "code",
      "source": "class Layer:\n    def __init__(self,nNodesCurrent, nNodesNext, activationF):\n        self.nodesNo=nNodesCurrent\n        self.activations = np.zeros([nNodesCurrent,1])\n        self.activationF=activationF\n    \n        if nNodesNext==0:\n            self.weights=None\n        else:\n            self.weights=np.random.normal(0, 1, size=(nNodesCurrent,nNodesNext))",
      "execution_count": 108,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "foJcY9UbkqRp",
        "trusted": true,
        "_uuid": "e17771a8b78f252040509525f72dd6c69605adb8"
      },
      "cell_type": "code",
      "source": "class NeuralNet:\n    def __init__(self, totalLayers, noNodesList, activationFunctions):\n        self.totalLayers=totalLayers\n        self.noNodesList=noNodesList\n        self.layers = []\n        for i in range(totalLayers):\n            currentLayerNodes=noNodesList[i]\n            if i!=totalLayers-1:\n                nextLayerNodes=noNodesList[i+1]\n                ith_Layer=Layer(currentLayerNodes,nextLayerNodes,activationFunctions[i])\n            else:\n                ith_Layer=Layer(currentLayerNodes,0,activationFunctions[i])\n            self.layers.append(ith_Layer)#append output layer as none\n\n    def trainNetwork(self, data,outputLabels, batchSize, epochs, learningRate):\n        self.learningRate=learningRate\n        self.batchSize=batchSize;\n        \n        #normalize data\n#         data=((data-data.min(axis=0))/(data.max(axis=0)-data.min(axis=0)))\n        data=data/255\n#         data=((data-data.mean(axis=0))/(data.std(axis=0)))\n        \n        for x in range(epochs):\n            i=0  \n            while i<len(data):\n                self.error=0\n                self.forwardPropo(data[i:i+batchSize])#input\n                self.calculateError(outputLabels[i:i+batchSize])#output\n#                 print(\"==============Batch\",i,\"================\")\n                self.back_pass(outputLabels[i:i+batchSize])\n                i+=batchSize\n            self.error /= batchSize\n            print(\"Error: \",x, self.error)\n          \n    def forwardPropo(self, inputs):\n        self.layers[0].activations =inputs\n        for i in range(self.totalLayers-1):\n#             print(self.layers[i].activations.shape,self.layers[i].weights.shape,)\n            temp=np.matmul(self.layers[i].activations,self.layers[i].weights)  \n           # print(temp)\n#             print(\"==============================\")\n            if self.layers[i+1].activationF == \"sigmoid\":\n                self.layers[i+1].activations = self.sigmoid(temp)\n            elif self.layers[i+1].activationF == \"softmax\":\n                self.layers[i+1].activations = self.softmax(temp)\n            elif self.layers[i+1].activationF == \"relu\":\n                self.layers[i+1].activations = self.relu(temp)\n            elif self.layers[i+1].activationF == \"tanh\":\n                self.layers[i+1].activations = self.tanh(temp)\n            else:\n                self.layers[i+1].activations = temp\n            # print(self.layers[self.totalLayers-1].activations)\n        \n    def calculateError(self,labels):\n#         print(labels.shape)\n        if len(labels[0]) != self.layers[self.totalLayers-1].nodesNo:\n            print (\"Error: Label is not of the same shape as output layer.\")\n            print(\"Label: \", len(labels), \" : \", len(labels[0]))\n            print(\"Out: \", len(self.layers[self.totalLayers-1].activations), \" : \", len(self.layers[self.totalLayers-1].activations[0]))\n            return\n        self.error += np.negative(np.sum(np.multiply(labels, np.log(self.layers[self.totalLayers-1].activations))))\n    \n    def back_pass(self, labels):\n        # if self.cost_function == \"cross_entropy\" and self.layers[self.num_layers-1].activation_function == \"softmax\":\n        targets = labels\n        i = self.totalLayers-1\n        y = self.layers[i].activations\n        \n        delta=(y-targets)\n        deltaw = np.dot(self.layers[i-1].activations.T, delta)/self.batchSize\n        new_weights = self.layers[i-1].weights - self.learningRate * deltaw\n        for i in range(i-1, 0, -1):\n            \n            sigPrime = self.sigmoidDerivative(np.matmul(self.layers[i-1].activations,self.layers[i-1].weights))\n#             reluPrime = self.relu_derivative(np.matmul(self.layers[i-1].activations,self.layers[i-1].weights))            \n#             tanPrime = self.tanhDerivative(np.matmul(self.layers[i-1].activations,self.layers[i-1].weights))\n            \n            delta=np.multiply(sigPrime,delta.dot(self.layers[i].weights.T))\n            deltaw = np.dot(self.layers[i-1].activations.T, delta)/self.batchSize\n\n            self.layers[i].weights = new_weights\n            new_weights = self.layers[i-1].weights - self.learningRate * deltaw\n        self.layers[0].weights = new_weights\n            \n    def check_accuracy(self, inputs, labels):\n        self.batchSize = len(inputs)\n        self.forwardPropo(inputs)\n        a = self.layers[self.totalLayers-1].activations\n        print(len(a))\n        total=0\n        correct=0\n        for i in range(len(a)):\n            total += 1\n            al = a[i].tolist()\n#             print(al,labels[i])\n            if labels[i][al.index(max(al))] == 1:\n                correct += 1\n        print(correct)\n        print(\"Accuracy: \", correct*100/total)\n    \n    def sigmoid(self, x):\n        return np.divide(1, np.add(1, np.exp(np.negative(x))))\n    \n    def sigmoidDerivative(self,x):\n        return (self.sigmoid(x)*(1-self.sigmoid(x)))\n    \n    def relu(self, x):\n        return x * (x > 0)#/700\n    \n    def relu_derivative(self,X):\n        return 1. * (X > 0)\n    \n    def softmax(self, x):\n        exp = np.exp(x)\n        if isinstance(x[0], np.ndarray):\n            return exp/np.sum(exp, axis=1, keepdims=True)\n        else:\n            return exp/np.sum(exp, keepdims=True)\n\n    def tanh(self, x):\n        return np.tanh(x)\n    \n    def tanhDerivative(self,x):\n        return 1.0 - np.tanh(x) ** 2",
      "execution_count": 171,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SQ5KUVammIaS",
        "trusted": true,
        "_uuid": "29bb65a597a19af437582c16d930790b7a792a83"
      },
      "cell_type": "code",
      "source": "data=pd.read_csv(\"../input/apparel-trainval.csv\").values",
      "execution_count": 42,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f382776340b5b2ceb8352f6cd8511ad0835d8a1e"
      },
      "cell_type": "code",
      "source": "def getOneHotLabels(data,k):\n    one_hot_labels = np.zeros((len(data), k))\n    for i in range(len(data)):  \n        one_hot_labels[i,data[i,0]] = 1\n    return one_hot_labels",
      "execution_count": 172,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Ievzm6hptydg",
        "trusted": true,
        "_uuid": "55c624c07d44df9d4ae46f7c997f640163305b64"
      },
      "cell_type": "code",
      "source": "train,test=splitTrainTest(data,80)\noneHotLabelsTrain=getOneHotLabels(train,10)\noneHotLabelsTest=getOneHotLabels(test,10)\nprint(train.shape)\ntrainInputs=train[:,1:]\ntestInputs=test[:,1:]\nprint(trainInputs.shape)\nnn=NeuralNet(4,[784,16,16,10],[None,\"sigmoid\",\"sigmoid\",\"softmax\"])\n# nn=NeuralNet(4,[784,16,16,10],[None,\"tanh\",\"tanh\",\"softmax\"])\n# nn=NeuralNet(4,[784,16,16,10],[None,\"relu\",\"relu\",\"softmax\"])",
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(48000, 785)\n(48000, 784)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1938
        },
        "colab_type": "code",
        "id": "W-_GNGBct1aP",
        "outputId": "0be1d7d6-ca47-4882-a0d8-2a7005477d07",
        "scrolled": false,
        "trusted": true,
        "_uuid": "a592ac87c807866038be87d3b90bb2852d31e66a"
      },
      "cell_type": "code",
      "source": "# nn.trainNetwork(trainInputs,oneHotLabelsTrain,100,50,0.01)\nnn.trainNetwork(trainInputs,oneHotLabelsTrain,64,50,0.1)\n\nnn.check_accuracy( testInputs, oneHotLabelsTest)",
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Error:  0 1.1917104599122805\nError:  1 0.9055784486982013\nError:  2 0.80134140524004\nError:  3 0.7489179120202868\nError:  4 0.7029784567273103\nError:  5 0.6710474267856397\nError:  6 0.6483171086153241\nError:  7 0.6284838142368887\nError:  8 0.6179170255694653\nError:  9 0.6119074205054404\nError:  10 0.6008123783402415\nError:  11 0.5888943205678343\nError:  12 0.5790301170541488\nError:  13 0.5713148801360531\nError:  14 0.5632796151370592\nError:  15 0.5509785713830453\nError:  16 0.5379543405043368\nError:  17 0.5271498804779837\nError:  18 0.5182222824422325\nError:  19 0.5103022085837647\nError:  20 0.502810092264328\nError:  21 0.49526153126726113\nError:  22 0.48754903935839244\nError:  23 0.47978963058680274\nError:  24 0.47191456454749925\nError:  25 0.46363806053645873\nError:  26 0.45475255304626977\nError:  27 0.445323631090673\nError:  28 0.4355312555048888\nError:  29 0.42556295640453917\nError:  30 0.41562102657330924\nError:  31 0.4059050886468479\nError:  32 0.3965645877675605\nError:  33 0.38765465679719185\nError:  34 0.379127988915703\nError:  35 0.370876216909824\nError:  36 0.3628011051421093\nError:  37 0.35487047750297673\nError:  38 0.3471277912013241\nError:  39 0.3396631468567697\nError:  40 0.3325742166703194\nError:  41 0.32593772992594944\nError:  42 0.31979666530804457\nError:  43 0.31415990736244936\nError:  44 0.3090092211134051\nError:  45 0.3043089116161532\nError:  46 0.30001481415360876\nError:  47 0.2960808297930629\nError:  48 0.29246264010806133\nError:  49 0.28911916263883064\n12000\n9748\nAccuracy:  81.23333333333333\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:103: RuntimeWarning: overflow encountered in exp\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ud9EgMOW0yU6",
        "trusted": true,
        "_uuid": "6d882d143dbbca167b6fbc09bc161dc5e86eb80e"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c60107b700878808af4e23cb1ae4490d5d4c278a"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Ass5.ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}