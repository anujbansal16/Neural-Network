{
  "cells": [
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DlYwwlVNYTQJ",
        "trusted": true,
        "_uuid": "d78c30fb1ae3fbe28508c51b0388e9a6e7ac1f54"
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\n",
      "execution_count": 170,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3El9g_GXYayN",
        "trusted": true,
        "_uuid": "c902470f1c4c7ca9b8a9f330a312f7869c646e73"
      },
      "cell_type": "code",
      "source": "def splitTrainTest(data,percent):\n    total=len(data)\n    trainTotal=int(total*percent*0.01)\n    testTotal=total-trainTotal\n    return (data[0:trainTotal],data[trainTotal:total])",
      "execution_count": 171,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5q_LrpcRa4sm",
        "trusted": true,
        "_uuid": "296710bb41c7f337c77f17726e26c77c4c04e2c0"
      },
      "cell_type": "code",
      "source": "class Layer:\n    def __init__(self,nNodesCurrent, nNodesNext, activationF):\n        self.nodesNo=nNodesCurrent\n        self.activations = np.zeros([nNodesCurrent,1])\n        self.activationF=activationF\n    \n        if nNodesNext==0:\n            self.weights=None\n        else:\n            self.weights=np.random.normal(0, 1, size=(nNodesCurrent,nNodesNext))",
      "execution_count": 172,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "foJcY9UbkqRp",
        "trusted": true,
        "_uuid": "e17771a8b78f252040509525f72dd6c69605adb8"
      },
      "cell_type": "code",
      "source": "class NeuralNet:\n    def __init__(self, totalLayers, noNodesList, activationFunctions):\n        self.totalLayers=totalLayers\n        self.noNodesList=noNodesList\n        self.layers = []\n        for i in range(totalLayers):\n            currentLayerNodes=noNodesList[i]\n            if i!=totalLayers-1:\n                nextLayerNodes=noNodesList[i+1]\n                ith_Layer=Layer(currentLayerNodes,nextLayerNodes,activationFunctions[i])\n            else:\n                ith_Layer=Layer(currentLayerNodes,0,activationFunctions[i])\n            self.layers.append(ith_Layer)#append output layer as none\n\n    def trainNetwork(self, data,outputLabels, batchSize, epochs, learningRate):\n        self.learningRate=learningRate\n        self.batchSize=batchSize;\n        \n        #normalize data\n        data=((data-data.min(axis=0))/(data.max(axis=0)-data.min(axis=0)))\n                \n        for x in range(epochs):\n            i=0  \n            while i<len(data):\n                self.error=0\n                self.forwardPropo(data[i:i+batchSize])#input\n                self.calculateError(outputLabels[i:i+batchSize])#output\n#                 print(\"==============Batch\",i,\"================\")\n                self.back_pass(outputLabels[i:i+batchSize])\n                i+=batchSize\n            self.error /= batchSize\n            print(\"Error: \", self.error)\n          \n    def forwardPropo(self, inputs):\n        self.layers[0].activations =inputs\n        for i in range(self.totalLayers-1):\n#             print(self.layers[i].activations.shape,self.layers[i].weights.shape,)\n            temp=np.matmul(self.layers[i].activations,self.layers[i].weights)  \n#           print(\"==============================\")\n            if self.layers[i+1].activationF == \"sigmoid\":\n                self.layers[i+1].activations = self.sigmoid(temp)\n            elif self.layers[i+1].activationF == \"softmax\":\n                self.layers[i+1].activations = self.softmax(temp)\n            elif self.layers[i+1].activationF == \"relu\":\n                self.layers[i+1].activations = self.relu(temp)\n            elif self.layers[i+1].activationF == \"tanh\":\n                self.layers[i+1].activations = self.tanh(temp)\n            else:\n                self.layers[i+1].activations = temp\n            # print(self.layers[self.totalLayers-1].activations)\n        \n    def calculateError(self,labels):\n#         print(labels.shape)\n        if len(labels[0]) != self.layers[self.totalLayers-1].nodesNo:\n            print (\"Error: Label is not of the same shape as output layer.\")\n            print(\"Label: \", len(labels), \" : \", len(labels[0]))\n            print(\"Out: \", len(self.layers[self.totalLayers-1].activations), \" : \", len(self.layers[self.totalLayers-1].activations[0]))\n            return\n        self.error += np.negative(np.sum(np.multiply(labels, np.log(self.layers[self.totalLayers-1].activations))))\n    \n    def back_pass(self, labels):\n        # if self.cost_function == \"cross_entropy\" and self.layers[self.num_layers-1].activation_function == \"softmax\":\n        targets = labels\n        i = self.totalLayers-1\n        y = self.layers[i].activations\n        \n        delta=(y-targets)#/targets.shape[0]\n        deltaw = np.dot(self.layers[i-1].activations.T, delta)/self.batchSize\n        new_weights = self.layers[i-1].weights - self.learningRate * deltaw\n        for i in range(i-1, 0, -1):\n            \n            sigPrime = self.sigmoidDerivative(self.layers[i].activations)\n            delta=np.multiply(sigPrime,delta.dot(self.layers[i].weights.T))\n            deltaw = np.dot(self.layers[i-1].activations.T, delta)/self.batchSize\n\n            self.layers[i].weights = new_weights\n            new_weights = self.layers[i-1].weights - self.learningRate * deltaw\n        self.layers[0].weights = new_weights\n            \n    def check_accuracy(self, inputs, labels):\n        self.batchSize = len(inputs)\n        self.forwardPropo(inputs)\n        a = self.layers[self.totalLayers-1].activations\n        print(len(a))\n        total=0\n        correct=0\n        for i in range(len(a)):\n            total += 1\n            al = a[i].tolist()\n            if labels[i][al.index(max(al))] == 1:\n                correct += 1\n        print(correct)\n        print(\"Accuracy: \", correct*100/total)\n    \n    def sigmoid(self, x):\n        return np.divide(1, np.add(1, np.exp(np.negative(x))))\n    \n    def sigmoidDerivative(self,x):\n        return (self.sigmoid(x)*(1-self.sigmoid(x)))\n    \n    def relu(self, x):\n        x[x<0] = 0\n        return x\n    \n    def relu_derivative(X):\n        return 1. * (X > 0)\n    \n    def softmax(self, x):\n        exp = np.exp(x)\n        if isinstance(x[0], np.ndarray):\n            return exp/np.sum(exp, axis=1, keepdims=True)\n        else:\n            return exp/np.sum(exp, keepdims=True)\n\n    def tanh(self, x):\n        return np.tanh(x)\n    \n    def tanhDerivative(x):\n        return 1.0 - np.tanh(x) ** 2",
      "execution_count": 173,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SQ5KUVammIaS",
        "trusted": true,
        "_uuid": "29bb65a597a19af437582c16d930790b7a792a83"
      },
      "cell_type": "code",
      "source": "data=pd.read_csv(\"../input/apparel-trainval.csv\").values",
      "execution_count": 174,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f382776340b5b2ceb8352f6cd8511ad0835d8a1e"
      },
      "cell_type": "code",
      "source": "def getOneHotLabels(data,k):\n    one_hot_labels = np.zeros((len(data), k))\n    for i in range(len(data)):  \n        one_hot_labels[i,data[i,0]] = 1\n    return one_hot_labels",
      "execution_count": 175,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Ievzm6hptydg",
        "trusted": true,
        "_uuid": "55c624c07d44df9d4ae46f7c997f640163305b64"
      },
      "cell_type": "code",
      "source": "train,test=splitTrainTest(data,80)\noneHotLabelsTrain=getOneHotLabels(train,10)\noneHotLabelsTest=getOneHotLabels(test,10)\nprint(train.shape)\ntrainInputs=train[:,1:]\ntestInputs=test[:,1:]\nprint(trainInputs.shape)\n# nn=NeuralNet(4,[784,16,16,10],[\"sigmoid\",\"sigmoid\",\"sigmoid\",\"softmax\"])\nnn=NeuralNet(3,[784,500,10],[None,\"sigmoid\",\"softmax\"])",
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(48000, 785)\n(48000, 784)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1938
        },
        "colab_type": "code",
        "id": "W-_GNGBct1aP",
        "outputId": "0be1d7d6-ca47-4882-a0d8-2a7005477d07",
        "scrolled": false,
        "trusted": true,
        "_uuid": "a592ac87c807866038be87d3b90bb2852d31e66a"
      },
      "cell_type": "code",
      "source": "nn.trainNetwork(trainInputs,oneHotLabelsTrain,100,200,0.01)\nnn.check_accuracy( testInputs, oneHotLabelsTest)",
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Error:  2.4485487329703775\nError:  1.7617810079175198\nError:  1.4497582425450468\nError:  1.2876362370135856\nError:  1.161496219885037\nError:  1.0782030744162285\nError:  1.0235548451049408\nError:  0.9940743116865158\nError:  0.9912080446361867\nError:  1.0000475346689386\nError:  1.0170530030821012\nError:  1.0404956780255712\nError:  1.0642051107613772\nError:  1.0736596086716426\nError:  1.0632503686339518\nError:  1.0565771489896063\nError:  1.071497720014611\nError:  1.0997860101916666\nError:  1.1227154983194367\nError:  1.1199742671336042\nError:  1.085593205661077\nError:  1.0325935487782774\nError:  0.9818501406645294\nError:  0.9458598183224634\nError:  0.9278020493195618\nError:  0.9268612663118698\nError:  0.9423379842004814\nError:  0.9664177221103071\nError:  0.9941387641148526\nError:  1.0226459090703204\nError:  1.043614457702835\nError:  1.052339071199499\nError:  1.051673838648337\nError:  1.0471656917433347\nError:  1.0439901500293585\nError:  1.0353168747053474\nError:  1.0168772292786097\nError:  0.9942374755775503\nError:  0.9759815949702406\nError:  0.9608180670275752\nError:  0.9390883105344021\nError:  0.9131867314028116\nError:  0.8931704941208602\nError:  0.8766596630239095\nError:  0.8602294301972984\nError:  0.8454589175053093\nError:  0.8345046010961483\nError:  0.8267308256844043\nError:  0.820581676139025\nError:  0.8198951014827404\nError:  0.8292938430097148\nError:  0.8451230523475547\nError:  0.857765082983393\nError:  0.8632825350491546\nError:  0.861038212254813\nError:  0.8554841050054143\nError:  0.853345605214271\nError:  0.8553924618873353\nError:  0.8579146017728664\nError:  0.8550475457966111\nError:  0.843548433010323\nError:  0.8244869278366199\nError:  0.7990973464943488\nError:  0.7698331934126622\nError:  0.7399651839116735\nError:  0.709850293904365\nError:  0.6782602160432006\nError:  0.6458128225664916\nError:  0.6137033896048817\nError:  0.5823859525812302\nError:  0.554176626802702\nError:  0.5314300091219822\nError:  0.5145126099355813\nError:  0.5018160013587911\nError:  0.4916940144142467\nError:  0.4840057754101316\nError:  0.4796167519384013\nError:  0.47813667287102163\nError:  0.4769418742632817\nError:  0.47402675759161256\nError:  0.4708075200706193\nError:  0.4710866443250852\nError:  0.4764831044954815\nError:  0.4851324820648978\nError:  0.49448607272598444\nError:  0.5031933805573128\nError:  0.5102708927059159\nError:  0.5157143948488527\nError:  0.5201533899342577\nError:  0.5235553757124771\nError:  0.5252633404041219\nError:  0.5241831863902374\nError:  0.5192225280239792\nError:  0.5109442314169044\nError:  0.5018731424802058\nError:  0.4947098444650891\nError:  0.49099086258723595\nError:  0.4904043586941006\nError:  0.4917048118353945\nError:  0.4940170042105524\nError:  0.49638216062585444\nError:  0.4976362187392362\nError:  0.49746482196289177\nError:  0.4964970701220963\nError:  0.49557978683958276\nError:  0.4953403192563766\nError:  0.49607404198518784\nError:  0.4978510717584502\nError:  0.5008699961557713\nError:  0.505728367404459\nError:  0.5131347874272442\nError:  0.5230351209244084\nError:  0.5340791720991258\nError:  0.5436453230657813\nError:  0.5490445075439889\nError:  0.5498074480606775\nError:  0.5480737621896922\nError:  0.5458506841804336\nError:  0.5429865153992752\nError:  0.5374593194890087\nError:  0.5287910051559063\nError:  0.522914303936825\nError:  0.5233314269874456\nError:  0.5269955590312483\nError:  0.530168948569564\nError:  0.5307484232794626\nError:  0.5277258326646386\nError:  0.5224948397564002\nError:  0.5195295536723179\nError:  0.5187699818821829\nError:  0.5182364146060233\nError:  0.5201607787692671\nError:  0.5214321843527799\nError:  0.5179352511305371\nError:  0.5159850299820147\nError:  0.5194312548371035\nError:  0.518478288345662\nError:  0.5136533440037132\nError:  0.5067029104389349\nError:  0.49463494368355465\nError:  0.4745644111139029\nError:  0.4470706347131053\nError:  0.42933179761577944\nError:  0.42127796942257545\nError:  0.4183964089449937\nError:  0.42034213154630323\nError:  0.42402381385863597\nError:  0.427987714167001\nError:  0.4334259735388383\nError:  0.4407388612045409\nError:  0.44752584884181523\nError:  0.45147565942837287\nError:  0.4518920335346799\nError:  0.4496854632846891\nError:  0.44672346865931173\nError:  0.44417142285574895\nError:  0.4421785953181741\nError:  0.44083056392421627\nError:  0.4407955992539361\nError:  0.442995167714885\nError:  0.44818821571949946\nError:  0.45627508102273573\nError:  0.4660902893201321\nError:  0.47623215882532205\nError:  0.48513595486665656\nError:  0.49154757236902497\nError:  0.49528415016408595\nError:  0.49874935371429374\nError:  0.5056195297089817\nError:  0.5176533484366643\nError:  0.5334119604819062\nError:  0.5484995483787334\nError:  0.5599101134417582\nError:  0.56773216471301\nError:  0.5727141526891704\nError:  0.5747903586435662\nError:  0.5767553716866519\nError:  0.579314002394141\nError:  0.5782166693427334\nError:  0.5726325546672486\nError:  0.5642023143463188\nError:  0.553637893089765\nError:  0.5420184149571083\nError:  0.531185693384646\nError:  0.522160266367702\nError:  0.5145378216169108\nError:  0.5076652306298451\nError:  0.5015724605507725\nError:  0.49660639781713484\nError:  0.4929423645371038\nError:  0.49048442489875527\nError:  0.4887590654785741\nError:  0.48705094335869714\nError:  0.48500815475281184\nError:  0.4823875875938579\nError:  0.4791254514908724\nError:  0.4757835099665286\nError:  0.4732771817428278\nError:  0.4713932873602096\nError:  0.46986037371518863\n12000\n8966\nAccuracy:  74.71666666666667\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:96: RuntimeWarning: overflow encountered in exp\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ud9EgMOW0yU6",
        "trusted": true,
        "_uuid": "6d882d143dbbca167b6fbc09bc161dc5e86eb80e"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c60107b700878808af4e23cb1ae4490d5d4c278a"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Ass5.ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}